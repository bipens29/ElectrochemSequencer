{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import raw oligomer degradation data, normalize them, and plot them. Save the raw and normalized overlay plots for cathodic and anodic traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "import os\n",
    "\n",
    "# Parameters for folder path and peak detection\n",
    "oligomer_names = ['Oligomer 1', 'Oligomer 2', 'Oligomer 3', 'Oligomer 4', 'Oligomer 5', 'Oligomer 6', 'Oligomer 7', 'Oligomer 8', 'Oligomer 9', 'Oligomer 10', 'Oligomer 14']  # Add all the oligomer names you want to process\n",
    "base_folder_path = './Raw Electrochem Data'\n",
    "processed_data_folder_path = './Processed Electrochem Data'  # Corrected to reference the base directory\n",
    "prominence_value = 3e-6\n",
    "\n",
    "def process_file(file_path, prominence_value, invert_current=False):\n",
    "    # Processing logic remains unchanged\n",
    "    data = pd.read_csv(file_path, delimiter='\\t', skiprows=2, usecols=[2, 6], names=['Potential', 'Current'])\n",
    "    if invert_current:\n",
    "        data['Current'] = -data['Current']\n",
    "    peaks, _ = find_peaks(data['Current'], prominence=prominence_value)\n",
    "    peak_data = data.iloc[peaks]\n",
    "    internal_standard_candidates = peak_data[\n",
    "        (peak_data['Potential'] >= -0.15) & (peak_data['Potential'] <= 0.2) &\n",
    "        (peak_data['Current'] >= 5e-6) & (peak_data['Current'] <= 8.5e-6)\n",
    "    ]\n",
    "    if not internal_standard_candidates.empty:\n",
    "        internal_standard = internal_standard_candidates.iloc[0]\n",
    "    else:\n",
    "        raise ValueError(\"No internal standard candidates found within the specified range.\")\n",
    "    potential_shift = internal_standard['Potential']\n",
    "    current_shift = internal_standard['Current']\n",
    "    data['Normalized_Potential'] = data['Potential'] - potential_shift\n",
    "    data['Normalized_Current'] = data['Current'] / current_shift\n",
    "    return data, potential_shift, current_shift\n",
    "\n",
    "def plot_and_save(legend_entries, plot_dir, oligomer_name, plot_title, file_suffix,\n",
    "                  title_font_size=17, axis_label_font_size=24, axis_tick_font_size=24, legend_font_size=18):\n",
    "    \"\"\"\n",
    "    Plot and save the normalized data with individual control over font sizes.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8), dpi=300)\n",
    "    plt.rcParams.update({'font.size': axis_tick_font_size})\n",
    "\n",
    "    # Define custom colors\n",
    "    custom_colors = ['#4C4A59', '#E6B64E', '#0897B4', '#4CABA6', '#F2CDAC', '#13678A', \n",
    "                     '#F2CB05', '#45C4B0', '#0F91FF', '#F29325', '#F2D7AC', '#007172']\n",
    "\n",
    "    # Filter out \"Blank electrolyte\" and non-numeric time points before processing\n",
    "    unique_legend_entries = [\n",
    "        (entry[0], tuple(entry[1]), tuple(entry[2]), entry[3])\n",
    "        for entry in legend_entries if \"blank electrolyte\" not in entry[0].lower()\n",
    "    ]\n",
    "\n",
    "    # Filter and convert time points to integers\n",
    "    filtered_legend_entries = []\n",
    "    for entry in unique_legend_entries:\n",
    "        try:\n",
    "            time_label = int(entry[0].replace('min', ''))\n",
    "            filtered_legend_entries.append((time_label, entry[1], entry[2], entry[3]))\n",
    "        except ValueError:\n",
    "            print(f\"Skipping entry with non-numeric time point: {entry[0]}\")\n",
    "            continue\n",
    "\n",
    "    filtered_legend_entries.sort(key=lambda x: x[0])\n",
    "    \n",
    "    num_time_points = min(len(filtered_legend_entries), 20)  # Limit to 20 time points for readability\n",
    "    for i, entry in enumerate(filtered_legend_entries[:num_time_points]):\n",
    "        time_label, potential, current, oligomer = entry\n",
    "        plt.plot(potential, current, label=f'{time_label}', color=custom_colors[i % len(custom_colors)], linewidth=2)\n",
    "\n",
    "    # Add legend and labels to the plot with specified font sizes\n",
    "    plt.xlabel('Normalized Potential (V)', fontsize=axis_label_font_size)\n",
    "    plt.ylabel('Normalized Current (Relative)', fontsize=axis_label_font_size)\n",
    "    \n",
    "    # Set legend with two columns, placed at the top left corner\n",
    "    plt.legend(title=\"Minutes\", fontsize=legend_font_size, title_fontsize=legend_font_size, \n",
    "               loc='upper right', ncol=1)\n",
    "\n",
    "    # Turn off the grid\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Set the x-axis limits\n",
    "    plt.xlim(-0.2, 1.45)\n",
    "\n",
    "    # Save the plot with the oligomer name included\n",
    "    plot_path = os.path.join(plot_dir, f'{oligomer_name}_{file_suffix}.png')\n",
    "    plt.savefig(plot_path, bbox_inches='tight')  # Remove white spaces\n",
    "    print(f\"Plot saved to {plot_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_raw_and_save(raw_legend_entries, plot_dir, oligomer_name, plot_title, file_suffix,\n",
    "                      title_font_size=17, axis_label_font_size=24, axis_tick_font_size=24, legend_font_size=18):\n",
    "    \"\"\"\n",
    "    Plot and save the raw data with individual control over font sizes.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8), dpi=300)\n",
    "    plt.rcParams.update({'font.size': axis_tick_font_size})\n",
    "\n",
    "    # Define custom colors\n",
    "    custom_colors = ['#4C4A59', '#E6B64E', '#0897B4', '#4CABA6', '#F2CDAC', '#13678A', \n",
    "                     '#F2CB05', '#45C4B0', '#0F91FF', '#F29325', '#F2D7AC', '#007172']\n",
    "\n",
    "    # Filter out \"Blank electrolyte\" and non-numeric time points before processing\n",
    "    unique_legend_entries = [\n",
    "        (entry[0], tuple(entry[1]), tuple(entry[2]), entry[3])\n",
    "        for entry in raw_legend_entries if \"blank electrolyte\" not in entry[0].lower()\n",
    "    ]\n",
    "\n",
    "    # Filter and convert time points to integers\n",
    "    filtered_legend_entries = []\n",
    "    for entry in unique_legend_entries:\n",
    "        try:\n",
    "            time_label = int(entry[0].replace('min', ''))\n",
    "            filtered_legend_entries.append((time_label, entry[1], entry[2], entry[3]))\n",
    "        except ValueError:\n",
    "            print(f\"Skipping entry with non-numeric time point: {entry[0]}\")\n",
    "            continue\n",
    "\n",
    "    filtered_legend_entries.sort(key=lambda x: x[0])\n",
    "    \n",
    "    num_time_points = min(len(filtered_legend_entries), 20)  # Limit to 20 time points for readability\n",
    "    for i, entry in enumerate(filtered_legend_entries[:num_time_points]):\n",
    "        time_label, potential, current, oligomer = entry\n",
    "        plt.plot(potential, current, label=f'{time_label}', color=custom_colors[i % len(custom_colors)], linewidth=2)\n",
    "\n",
    "    # Add legend and labels to the plot with specified font sizes\n",
    "    plt.xlabel('Potential (V)', fontsize=axis_label_font_size)\n",
    "    plt.ylabel('Current (A)', fontsize=axis_label_font_size)\n",
    "    \n",
    "    # Set legend with two columns, placed at the top left corner\n",
    "    plt.legend(title=\"Minutes\", fontsize=legend_font_size, title_fontsize=legend_font_size, \n",
    "               loc='upper right', ncol=1)\n",
    "\n",
    "    # Turn off the grid\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Save the plot with the oligomer name included\n",
    "    plot_path = os.path.join(plot_dir, f'{oligomer_name}_{file_suffix}.png')\n",
    "    plt.savefig(plot_path, bbox_inches='tight')  # Remove white spaces\n",
    "    print(f\"Plot saved to {plot_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def run_and_save(base_folder_path, oligomer_names, prominence_value):\n",
    "    \"\"\"\n",
    "    Run the processing and save results for all oligomers in the given list.\n",
    "    \"\"\"\n",
    "    # Create the main directory for processed data\n",
    "    os.makedirs(processed_data_folder_path, exist_ok=True)\n",
    "    \n",
    "    for oligomer_name in oligomer_names:\n",
    "        oligomer_folder_path = os.path.join(processed_data_folder_path, oligomer_name)\n",
    "        os.makedirs(oligomer_folder_path, exist_ok=True)\n",
    "\n",
    "        anodic_result_dir = os.path.join(oligomer_folder_path, 'Normalized Anodic Trace Data')\n",
    "        cathodic_result_dir = os.path.join(oligomer_folder_path, 'Normalized Cathodic Trace Data')\n",
    "        plot_dir = os.path.join(oligomer_folder_path, 'Plots')\n",
    "\n",
    "        # Ensure all necessary directories are created\n",
    "        os.makedirs(anodic_result_dir, exist_ok=True)\n",
    "        os.makedirs(cathodic_result_dir, exist_ok=True)\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "        anodic_legend_entries = []\n",
    "        cathodic_legend_entries = []\n",
    "        raw_anodic_legend_entries = []\n",
    "        raw_cathodic_legend_entries = []\n",
    "\n",
    "        for root, _, files in os.walk(os.path.join(base_folder_path, oligomer_name)):\n",
    "            for file in files:\n",
    "                if file == 'DPV_Anodic_Trace.txt':\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    time_point = os.path.basename(os.path.dirname(file_path))\n",
    "\n",
    "                    if 'blank' in time_point.lower():\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        normalized_data, _, _ = process_file(file_path, prominence_value)\n",
    "                        \n",
    "                        anodic_legend_entries.append((time_point, list(normalized_data['Normalized_Potential']), list(normalized_data['Normalized_Current']), oligomer_name))\n",
    "                        raw_anodic_legend_entries.append((time_point, list(normalized_data['Potential']), list(normalized_data['Current']), oligomer_name))\n",
    "\n",
    "                        normalized_data_path = os.path.join(anodic_result_dir, f'{oligomer_name}_{time_point}_Anodic_Normalized.csv')\n",
    "                        normalized_data.to_csv(normalized_data_path, index=False)\n",
    "\n",
    "                        print(f\"Processed and saved: {normalized_data_path}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "                elif file == 'DPV_Cathodic_Trace.txt':\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    time_point = os.path.basename(os.path.dirname(file_path))\n",
    "\n",
    "                    if 'blank' in time_point.lower():\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        normalized_data, _, _ = process_file(file_path, prominence_value, invert_current=True)\n",
    "                        \n",
    "                        cathodic_legend_entries.append((time_point, list(normalized_data['Normalized_Potential']), list(normalized_data['Normalized_Current']), oligomer_name))\n",
    "                        raw_cathodic_legend_entries.append((time_point, list(normalized_data['Potential']), list(normalized_data['Current']), oligomer_name))\n",
    "\n",
    "                        normalized_data_path = os.path.join(cathodic_result_dir, f'{oligomer_name}_{time_point}_Cathodic_Normalized.csv')\n",
    "                        normalized_data.to_csv(normalized_data_path, index=False)\n",
    "\n",
    "                        print(f\"Processed and saved: {normalized_data_path}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "        if not anodic_legend_entries:\n",
    "            print(f\"No valid anodic oligomer data found for {oligomer_name} to plot.\")\n",
    "        else:\n",
    "            plot_and_save(anodic_legend_entries, plot_dir, oligomer_name, f'Normalized DPV Anodic Traces for {oligomer_name}', 'Normalized_Anodic_trace_plot')\n",
    "            plot_raw_and_save(raw_anodic_legend_entries, plot_dir, oligomer_name, f'Raw DPV Anodic Traces for {oligomer_name}', 'Raw_Anodic_trace_plot')\n",
    "        \n",
    "        if not cathodic_legend_entries:\n",
    "            print(f\"No valid cathodic oligomer data found for {oligomer_name} to plot.\")\n",
    "        else:\n",
    "            plot_and_save(cathodic_legend_entries, plot_dir, oligomer_name, f'Normalized DPV Cathodic Traces for {oligomer_name}', 'Normalized_Cathodic_trace_plot')\n",
    "            plot_raw_and_save(raw_cathodic_legend_entries, plot_dir, oligomer_name, f'Raw DPV Cathodic Traces for {oligomer_name}', 'Raw_Cathodic_trace_plot')\n",
    "\n",
    "# Run the function to process files and save the results for all oligomers\n",
    "run_and_save(base_folder_path, oligomer_names, prominence_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import normalized anodic and cathodic trace data for specified oligomers, perform curve fitting, and save the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.optimize import curve_fit\n",
    "import os\n",
    "\n",
    "# Define parameters\n",
    "oligomers = ['Oligomer 1', 'Oligomer 2', 'Oligomer 3', 'Oligomer 4', 'Oligomer 5', 'Oligomer 6', \n",
    "             'Oligomer 7', 'Oligomer 8', 'Oligomer 9', 'Oligomer 10', 'Oligomer 14']\n",
    "data_types = ['Anodic', 'Cathodic']\n",
    "prominence_value = 0.0001\n",
    "threshold_value = 0.1\n",
    "min_width_value = 5\n",
    "min_distance_value = 5\n",
    "fit_range = 5\n",
    "tolerance = 0.1  # Tolerance for peak matching\n",
    "baseline_threshold = 0.02  # Threshold to consider a peak above baseline in 0 min data\n",
    "dpi_value = 300  # Set higher DPI for better resolution\n",
    "\n",
    "# Plot customization parameters\n",
    "font_size_title = 23\n",
    "font_size_legend = 18\n",
    "font_size_axis_title = 23\n",
    "font_size_axis_marks = 23\n",
    "line_thickness = 3\n",
    "\n",
    "# Define Gaussian function for curve fitting\n",
    "def gaussian(x, amp, cen, wid):\n",
    "    return amp * np.exp(-(x - cen)**2 / (2 * wid**2))\n",
    "\n",
    "# Function to detect peaks\n",
    "def detect_peaks(data, prominence, threshold, min_width, min_distance):\n",
    "    peaks, properties = find_peaks(data['Normalized_Current'], prominence=prominence, width=min_width, height=threshold, distance=min_distance)\n",
    "    return peaks, properties\n",
    "\n",
    "# Function to fit Gaussian curves around peaks iteratively\n",
    "def fit_gaussians(data, peaks):\n",
    "    fitted_peaks = []\n",
    "    for peak in peaks:\n",
    "        lower_bound = max(0, peak - fit_range)\n",
    "        upper_bound = min(len(data) - 1, peak + fit_range)\n",
    "        x_fit = data['Normalized_Potential'].iloc[lower_bound:upper_bound]\n",
    "        y_fit = data['Normalized_Current'].iloc[lower_bound:upper_bound]\n",
    "\n",
    "        popt = [y_fit.max(), x_fit[y_fit.idxmax()], 0.1]\n",
    "        bounds = ([0, x_fit.min(), 0], [np.inf, x_fit.max(), np.inf])\n",
    "\n",
    "        try:\n",
    "            popt, _ = curve_fit(gaussian, x_fit, y_fit, p0=popt, bounds=bounds)\n",
    "            fitted_peaks.append(popt)\n",
    "        except RuntimeError:\n",
    "            continue\n",
    "\n",
    "    # Sort fitted peaks by their central potential (second element in popt)\n",
    "    fitted_peaks.sort(key=lambda x: x[1])\n",
    "\n",
    "    return fitted_peaks\n",
    "\n",
    "# Function to process a single file and detect peaks\n",
    "def process_file(file_path, prominence, threshold, min_width, min_distance):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None, None\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    data = pd.read_csv(file_path)\n",
    "    peaks, properties = detect_peaks(data, prominence, threshold, min_width, min_distance)\n",
    "    return data, peaks\n",
    "\n",
    "# Function to plot data with detected peaks and Gaussian fits and save the plot\n",
    "def plot_data_with_peaks_and_fits(data, peaks, fits, title, save_path):\n",
    "    plt.figure(figsize=(12, 8), dpi=dpi_value)\n",
    "    plt.plot(data['Normalized_Potential'], data['Normalized_Current'], label='Normalized Data', linewidth=line_thickness)\n",
    "    plt.scatter(data['Normalized_Potential'].iloc[peaks], data['Normalized_Current'].iloc[peaks], color='red', label='Detected Peaks')\n",
    "\n",
    "    # Sort fits by their central potential (x value)\n",
    "    fits.sort(key=lambda x: x[1])\n",
    "\n",
    "    for popt in fits:\n",
    "        x_fit = np.linspace(data['Normalized_Potential'].min(), data['Normalized_Potential'].max(), 1000)\n",
    "        y_fit = gaussian(x_fit, *popt)\n",
    "        peak_x = popt[1]\n",
    "        peak_y = popt[0]\n",
    "        plt.plot(x_fit, y_fit, linestyle='--', label=f'(x={peak_x:.2f}, y={peak_y:.2f})', linewidth=line_thickness)\n",
    "\n",
    "    plt.xlabel('Normalized Potential (V)', fontsize=font_size_axis_title)\n",
    "    plt.ylabel('Normalized Current (Relative)', fontsize=font_size_axis_title)\n",
    "    plt.title(title, fontsize=font_size_title)\n",
    "    plt.legend(loc='best', fontsize=font_size_legend)\n",
    "    plt.xticks(fontsize=font_size_axis_marks)\n",
    "    plt.yticks(fontsize=font_size_axis_marks)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot without displaying it in the notebook\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()  # Close the plot to free up memory\n",
    "\n",
    "# Function to process all files in the specified directory\n",
    "def process_directory(directory_path, oligomer_name, data_type, prominence, threshold, min_width, min_distance, output_folder, time_points):\n",
    "    all_peak_data = []  # List to store peak data for all files\n",
    "\n",
    "    # Process the 0 min data to identify the internal standard peak\n",
    "    file_name = f'{oligomer_name}_0 min_{data_type}_Normalized.csv'\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "    data_0min, peaks_0min = process_file(file_path, prominence, threshold, min_width, min_distance)\n",
    "    if data_0min is None:\n",
    "        return\n",
    "    internal_standard_peak = [p for p in peaks_0min if abs(data_0min['Normalized_Potential'].iloc[p]) < tolerance and data_0min['Normalized_Current'].iloc[p] > 0.9]\n",
    "    plot_data_with_peaks_and_fits(data_0min, internal_standard_peak, [], f'Normalized DPV Trace - 0 min', os.path.join(output_folder, f'{oligomer_name}_0 min_{data_type}_Normalized.png'))\n",
    "\n",
    "    # Store peak data\n",
    "    for p in internal_standard_peak:\n",
    "        all_peak_data.append([oligomer_name, '0 min', data_0min['Normalized_Potential'].iloc[p], data_0min['Normalized_Current'].iloc[p]])\n",
    "\n",
    "    # Process other time points to detect peaks and fit Gaussians\n",
    "    for time_point in time_points[1:]:  # Exclude 0 min as it's already processed\n",
    "        file_name = f'{oligomer_name}_{time_point}_{data_type}_Normalized.csv'\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        data, peaks = process_file(file_path, prominence, threshold, min_width, min_distance)\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        current_fits = []\n",
    "        for peak in peaks:\n",
    "            if data['Normalized_Current'].iloc[peak] > baseline_threshold:\n",
    "                lower_bound = max(0, peak - fit_range)\n",
    "                upper_bound = min(len(data) - 1, peak + fit_range)\n",
    "                x_fit = data['Normalized_Potential'].iloc[lower_bound:upper_bound]\n",
    "                y_fit = data['Normalized_Current'].iloc[lower_bound:upper_bound]\n",
    "                try:\n",
    "                    p0 = [y_fit.max(), x_fit[y_fit.idxmax()], 0.1]\n",
    "                    bounds = ([0, x_fit.min(), 0], [np.inf, x_fit.max(), np.inf])\n",
    "                    popt, _ = curve_fit(gaussian, x_fit, y_fit, p0=p0, bounds=bounds)\n",
    "                    current_fits.append(popt)\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "\n",
    "        plot_title = f'Normalized DPV Trace with Detected Peaks - {time_point}'\n",
    "        plot_path = os.path.join(output_folder, f'{oligomer_name}_{time_point}_{data_type}_Normalized.png')\n",
    "        plot_data_with_peaks_and_fits(data, peaks, current_fits, plot_title, plot_path)\n",
    "\n",
    "        # Store peak data\n",
    "        for fit in current_fits:\n",
    "            all_peak_data.append([oligomer_name, time_point, fit[1], fit[0]])\n",
    "\n",
    "    # Save all peak data to an Excel file in ascending order of time points\n",
    "    peak_data_df = pd.DataFrame(all_peak_data, columns=['Oligomer Name', 'Time Point', 'Potential (V)', 'Current (Relative)'])\n",
    "    peak_data_df['Time Point'] = pd.Categorical(peak_data_df['Time Point'], categories=time_points, ordered=True)\n",
    "    peak_data_df = peak_data_df.sort_values('Time Point')\n",
    "    peak_data_df.to_excel(os.path.join(output_folder, f'{oligomer_name}_peak_data.xlsx'), index=False)\n",
    "\n",
    "# Main processing loop for all oligomers and data types\n",
    "base_processed_folder_path = './Processed Electrochem Data'\n",
    "\n",
    "for oligomer in oligomers:\n",
    "    for data_type in data_types:\n",
    "        # Update the folder path to read normalized data from the \"Processed Electrochem Data\" folder\n",
    "        folder_path = f'{base_processed_folder_path}/{oligomer}/Normalized {data_type} Trace Data/'\n",
    "        # Update the output folder to save processed data\n",
    "        output_folder = os.path.join(base_processed_folder_path, oligomer, f'{data_type} Curve Fitted Data')\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        time_points = ['0 min', '10 min', '20 min', '30 min', '40 min', '50 min', '65 min', '80 min', '95 min', '110 min', '130 min', '150 min']\n",
    "        process_directory(folder_path, oligomer, data_type, prominence_value, threshold_value, min_width_value, min_distance_value, output_folder, time_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This code processes the peak current data from curve fitting for each oligomer (both anodic and cathodic), and filters the peak values for S1, S2, S3, and S4, and also makes an assignment of Monomers (M1, M2, M3, M4) to the S values. Then it consolidates this information in 4 excel files, two containing monomer assignments for each oligomer, and two containing S1, S2, S3, and S4 values for all oligomers for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "# Define the list of oligomers to process\n",
    "oligomers = ['Oligomer 1', 'Oligomer 2', 'Oligomer 3', 'Oligomer 4', 'Oligomer 5', 'Oligomer 6', \n",
    "             'Oligomer 7', 'Oligomer 8', 'Oligomer 9', 'Oligomer 10', 'Oligomer 14']\n",
    "\n",
    "# Base directory where the processed peak data is stored\n",
    "base_directory = './Processed Electrochem Data'\n",
    "\n",
    "# Define time points and the potential ranges for the monomers\n",
    "time_points = [0, 10, 20, 30, 40, 50, 65, 80, 95, 110, 130, 150]\n",
    "monomer_ranges = {\n",
    "    'M1': (0.60, 0.67),\n",
    "    'M2': (0.75, 0.85),\n",
    "    'M3': (0.90, 1.0),\n",
    "    'M4': (1.15, 1.30)\n",
    "}\n",
    "\n",
    "# Initialize lists to store assignment and S values data for all oligomers\n",
    "cathodic_assignments = []\n",
    "anodic_assignments = []\n",
    "cathodic_values = []\n",
    "anodic_values = []\n",
    "\n",
    "# Function to process the data for a given type (Cathodic/Anodic)\n",
    "def process_curve_data(file_path, oligomer_name, curve_type):\n",
    "    # Load the Excel file\n",
    "    df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
    "\n",
    "    # Print the column names to check for discrepancies\n",
    "    print(f\"Processing: {oligomer_name} ({curve_type})\")\n",
    "    print(\"Column names in the DataFrame:\", df.columns)\n",
    "\n",
    "    # Initialize the list to store data for the new DataFrame\n",
    "    data = []\n",
    "\n",
    "    # Get the unique oligomer name\n",
    "    oligomer_name = df['Oligomer Name'].unique()[0]\n",
    "\n",
    "    # Initialize a list to hold the first detection information for each monomer\n",
    "    first_detection = []\n",
    "\n",
    "    # Define the column names\n",
    "    potential_column = 'Potential (V)'\n",
    "    current_column = 'Current (Relative)'\n",
    "\n",
    "    # Step 1: Find the first detection time and current value for each monomer\n",
    "    for time in time_points:\n",
    "        # Filter data for the current time point\n",
    "        time_data = df[df['Time Point'] == f'{time} min']\n",
    "        \n",
    "        for _, row in time_data.iterrows():\n",
    "            potential = row[potential_column]\n",
    "            current = row[current_column]\n",
    "            for monomer, (low, high) in monomer_ranges.items():\n",
    "                if low <= potential <= high:\n",
    "                    first_detection.append((monomer, time, current))\n",
    "\n",
    "    # Step 2: Sort the first detection list based on time and current value\n",
    "    first_detection.sort(key=lambda x: (x[1], -x[2]))\n",
    "\n",
    "    # Step 3: Assign monomers to S1, S2, S3, and S4 based on the sorted list\n",
    "    assignments = {}\n",
    "    assigned_monomers = set()\n",
    "    for idx, (monomer, _, _) in enumerate(first_detection):\n",
    "        if monomer not in assigned_monomers:\n",
    "            assignments[f'S{len(assignments) + 1}'] = monomer\n",
    "            assigned_monomers.add(monomer)\n",
    "            if len(assignments) == 4:\n",
    "                break\n",
    "\n",
    "    # Step 4: Process the data to assign current values to S1, S2, S3, S4\n",
    "    for time in time_points:\n",
    "        # Filter data for the current time point\n",
    "        time_data = df[df['Time Point'] == f'{time} min']\n",
    "        \n",
    "        # Initialize the peak current values for S1, S2, S3, S4 as 0\n",
    "        s_values = {key: 0 for key in ['S1', 'S2', 'S3', 'S4']}\n",
    "        \n",
    "        for _, row in time_data.iterrows():\n",
    "            potential = row[potential_column]\n",
    "            current = row[current_column]\n",
    "            for s_label, monomer in assignments.items():\n",
    "                low, high = monomer_ranges[monomer]\n",
    "                if low <= potential <= high:\n",
    "                    s_values[s_label] = current\n",
    "        \n",
    "        # Append the row data to the list\n",
    "        data.append({\n",
    "            'Oligomer': oligomer_name,\n",
    "            'Time': time,\n",
    "            **s_values\n",
    "        })\n",
    "\n",
    "    # Convert the list to a DataFrame\n",
    "    output_df = pd.DataFrame(data)\n",
    "\n",
    "    # Rename the columns to include the monomer assignments\n",
    "    for s_label, monomer in assignments.items():\n",
    "        output_df.rename(columns={s_label: f\"{s_label}({monomer})\"}, inplace=True)\n",
    "\n",
    "    # Construct the output file path for processed data\n",
    "    output_directory = os.path.join(base_directory, oligomer_name, 'concentration-time data')\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    output_file_name = f\"Processed_{curve_type}_peak_data.xlsx\"\n",
    "    output_file_path = os.path.join(output_directory, output_file_name)\n",
    "\n",
    "    # Save the new DataFrame to an Excel file in the specified directory\n",
    "    output_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "    # Display the new DataFrame\n",
    "    display(output_df.head())\n",
    "\n",
    "    print(f\"Processed file saved as: {output_file_name}\\n\")\n",
    "\n",
    "    # Append the assignments to the respective list based on the curve type\n",
    "    for s_label, monomer in assignments.items():\n",
    "        if curve_type == 'Cathodic':\n",
    "            cathodic_assignments.append({\n",
    "                'Oligomer': oligomer_name,\n",
    "                'Assignment': s_label,\n",
    "                'Monomer': monomer\n",
    "            })\n",
    "        elif curve_type == 'Anodic':\n",
    "            anodic_assignments.append({\n",
    "                'Oligomer': oligomer_name,\n",
    "                'Assignment': s_label,\n",
    "                'Monomer': monomer\n",
    "            })\n",
    "\n",
    "    # Adjust the S1, S2, S3, S4 values for missing data across all time points\n",
    "    for s_label in ['S1', 'S2', 'S3', 'S4']:\n",
    "        if all(record[s_label] == 0 for record in data):\n",
    "            for record in data:\n",
    "                record[s_label] = float('nan')\n",
    "\n",
    "    # Append the S1, S2, S3, S4 values to the respective list based on the curve type\n",
    "    if curve_type == 'Cathodic':\n",
    "        cathodic_values.extend(data)\n",
    "    elif curve_type == 'Anodic':\n",
    "        anodic_values.extend(data)\n",
    "\n",
    "# Process each oligomer in the list for Cathodic data first and then Anodic data\n",
    "for oligomer in oligomers:\n",
    "    # Process Cathodic data\n",
    "    cathodic_file_path = os.path.join(base_directory, oligomer, 'Cathodic Curve Fitted Data', f'{oligomer}_peak_data.xlsx')\n",
    "    \n",
    "    if os.path.exists(cathodic_file_path):\n",
    "        process_curve_data(cathodic_file_path, oligomer, 'Cathodic')\n",
    "    else:\n",
    "        print(f\"Cathodic file not found for {oligomer}, skipping.\")\n",
    "\n",
    "    # Process Anodic data\n",
    "    anodic_file_path = os.path.join(base_directory, oligomer, 'Anodic Curve Fitted Data', f'{oligomer}_peak_data.xlsx')\n",
    "    \n",
    "    if os.path.exists(anodic_file_path):\n",
    "        process_curve_data(anodic_file_path, oligomer, 'Anodic')\n",
    "    else:\n",
    "        print(f\"Anodic file not found for {oligomer}, skipping.\")\n",
    "\n",
    "# After processing all oligomers, save the consolidated assignments to separate Excel files\n",
    "cathodic_assignments_df = pd.DataFrame(cathodic_assignments)\n",
    "anodic_assignments_df = pd.DataFrame(anodic_assignments)\n",
    "\n",
    "cathodic_values_df = pd.DataFrame(cathodic_values)\n",
    "anodic_values_df = pd.DataFrame(anodic_values)\n",
    "\n",
    "# Specify the file paths with the desired names\n",
    "cathodic_assignments_file_path = os.path.join(base_directory, 'Monomer_Assignments_Cathodic.xlsx')\n",
    "anodic_assignments_file_path = os.path.join(base_directory, 'Monomer_Assignments_Anodic.xlsx')\n",
    "\n",
    "cathodic_values_file_path = os.path.join(base_directory, 'Combined_Cathodic_Experimental_Peak_Currents.xlsx')\n",
    "anodic_values_file_path = os.path.join(base_directory, 'Combined_Anodic_Experimental_Peak_Currents.xlsx')\n",
    "\n",
    "# Save the DataFrames to the specified file paths\n",
    "cathodic_assignments_df.to_excel(cathodic_assignments_file_path, index=False)\n",
    "anodic_assignments_df.to_excel(anodic_assignments_file_path, index=False)\n",
    "\n",
    "cathodic_values_df.to_excel(cathodic_values_file_path, index=False)\n",
    "anodic_values_df.to_excel(anodic_values_file_path, index=False)\n",
    "\n",
    "print(f\"Cathodic assignments file saved as: {cathodic_assignments_file_path}\")\n",
    "print(f\"Anodic assignments file saved as: {anodic_assignments_file_path}\")\n",
    "print(f\"Cathodic values file saved as: {cathodic_values_file_path}\")\n",
    "print(f\"Anodic values file saved as: {anodic_values_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import experimental peak current data, perform curve fit, and normalize the curve-fitted data points, and save them on an excel file for PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "\n",
    "# Define file paths for cathodic and anodic data\n",
    "cathodic_file_path = './Processed Electrochem Data/Combined_Cathodic_Experimental_Peak_Currents.xlsx'\n",
    "anodic_file_path = './Processed Electrochem Data/Combined_Anodic_Experimental_Peak_Currents.xlsx'\n",
    "\n",
    "# Define file paths for monomer assignments\n",
    "cathodic_assignment_path = './Processed Electrochem Data/Monomer_Assignments_Cathodic.xlsx'\n",
    "anodic_assignment_path = './Processed Electrochem Data/Monomer_Assignments_Anodic.xlsx'\n",
    "\n",
    "# Load monomer assignments\n",
    "cathodic_assignments = pd.read_excel(cathodic_assignment_path)\n",
    "anodic_assignments = pd.read_excel(anodic_assignment_path)\n",
    "\n",
    "# Pivot the assignments DataFrame\n",
    "cathodic_assignments = cathodic_assignments.pivot(index='Oligomer', columns='Assignment', values='Monomer')\n",
    "anodic_assignments = anodic_assignments.pivot(index='Oligomer', columns='Assignment', values='Monomer')\n",
    "\n",
    "# Define the base directory for PCA Analysis\n",
    "pca_analysis_dir = './PCA Analysis'\n",
    "\n",
    "def process_oligomer_data(data_type, file_path, assignments):\n",
    "    # Load the known oligomer data from the Excel file\n",
    "    known_df = pd.read_excel(file_path)\n",
    "\n",
    "    # Define the intensity columns\n",
    "    intensity_columns = ['S1', 'S2', 'S3', 'S4']\n",
    "\n",
    "    # Filter out rows where all intensity values are NaN\n",
    "    known_df = known_df.dropna(subset=intensity_columns, how='all')\n",
    "\n",
    "    # Ensure 'Oligomer' column exists\n",
    "    if 'Oligomer' not in known_df.columns:\n",
    "        raise KeyError(\"'Oligomer' column not found in the DataFrame\")\n",
    "\n",
    "    # Define main directory based on data type\n",
    "    main_dir = os.path.join(pca_analysis_dir, data_type.capitalize())\n",
    "\n",
    "    # Define directories for saving plots and data\n",
    "    plots_dir = os.path.join(main_dir, 'Fitted plots')\n",
    "    excel_dir = os.path.join(main_dir, 'Fitted Peak Currents')\n",
    "    normalized_data_dir = os.path.join(main_dir, 'Fitted&Normalized Peak Currents')\n",
    "    normalized_plots_dir = os.path.join(main_dir, 'Fitted and Normalized Plots')\n",
    "\n",
    "    # Create directories if they do not exist\n",
    "    for dir_path in [plots_dir, excel_dir, normalized_data_dir, normalized_plots_dir]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    # Define a modified logistic fitting function\n",
    "    def modified_logistic(x, L, k, x_0):\n",
    "        return L / (1 + np.exp(-k * (x - x_0))) - L / (1 + np.exp(k * x_0))\n",
    "\n",
    "    # Plot settings\n",
    "    line_thickness = 4\n",
    "    axis_label_font_size = 28\n",
    "    axis_value_font_size = 28\n",
    "    legend_font_size = 28\n",
    "    marker_size = 30  # Marker size for data points\n",
    "\n",
    "    # Monomer colors\n",
    "    monomer_colors = {\n",
    "        'M1': '#F23030',\n",
    "        'M2': '#F2A71B',\n",
    "        'M3': '#A1C7E0',\n",
    "        'M4': '#009688'\n",
    "    }\n",
    "\n",
    "    # Initialize DataFrames to store fitted and normalized data\n",
    "    all_fitted_data_df = pd.DataFrame()\n",
    "    all_normalized_data_df = pd.DataFrame()\n",
    "\n",
    "    # Process each oligomer\n",
    "    for oligomer in known_df['Oligomer'].unique():\n",
    "        oligomer_data = known_df[known_df['Oligomer'] == oligomer].dropna(subset=['Time'])\n",
    "        \n",
    "        # Check if the assignment is available for this oligomer\n",
    "        if oligomer not in assignments.index:\n",
    "            print(f\"No assignment found for oligomer {oligomer}\")\n",
    "            continue\n",
    "\n",
    "        # Get the assignment for the current oligomer\n",
    "        assignment = assignments.loc[oligomer]\n",
    "\n",
    "        # Create a new figure for the fitted data\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        fitted_data_df = pd.DataFrame({'Time': oligomer_data['Time'].sort_values().unique()})\n",
    "        max_y_value = 0\n",
    "        \n",
    "        # Fit and plot each intensity column\n",
    "        for col in intensity_columns:\n",
    "            if col in assignment and oligomer_data[col].notna().any():\n",
    "                oligomer_data = oligomer_data.sort_values('Time')\n",
    "                time = oligomer_data['Time']\n",
    "                intensity = oligomer_data[col]\n",
    "                monomer = assignment[col]\n",
    "                \n",
    "                if len(time) > 3:\n",
    "                    try:\n",
    "                        p0 = [max(intensity), 1, np.median(time)]\n",
    "                        bounds = (0, [max(intensity) * 1.5, 10, time.max()])\n",
    "                        popt, _ = curve_fit(modified_logistic, time, intensity, p0=p0, bounds=bounds, maxfev=5000)\n",
    "                        \n",
    "                        time_smooth = np.linspace(time.min(), time.max(), 300)\n",
    "                        intensity_smooth = modified_logistic(time_smooth, *popt)\n",
    "                        \n",
    "                        plt.plot(time_smooth, intensity_smooth, label=f'{col}={monomer}', linewidth=line_thickness, color=monomer_colors[monomer])\n",
    "                        plt.scatter(time, intensity, marker='o', s=marker_size, color=monomer_colors[monomer])\n",
    "                        \n",
    "                        fitted_data_df[col] = modified_logistic(fitted_data_df['Time'], *popt)\n",
    "                        max_y_value = max(max_y_value, intensity.max(), intensity_smooth.max())\n",
    "                    except RuntimeError as e:\n",
    "                        print(f\"Could not fit data for oligomer {oligomer} and intensity {col}: {e}\")\n",
    "                else:\n",
    "                    print(f\"Insufficient data to fit for oligomer {oligomer} and intensity {col}\")\n",
    "\n",
    "        # Adjust the legend to only include the fitted plot lines\n",
    "        plt.xlabel('Time (min)', fontsize=axis_label_font_size)\n",
    "        plt.ylabel('Relative Concentration', fontsize=axis_label_font_size)\n",
    "        plt.legend(title=f'{oligomer}', fontsize=legend_font_size, title_fontsize=legend_font_size)  # Set legend title as the oligomer name\n",
    "        plt.xlim(0, 160)\n",
    "        plt.ylim(-0.05, max_y_value * 1.1)\n",
    "        plt.xticks(fontsize=axis_value_font_size)\n",
    "        plt.yticks(fontsize=axis_value_font_size)\n",
    "\n",
    "        # Adjust layout and remove white space around the plot\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plots_dir, f'{oligomer}_intensity_over_time.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        fitted_data_df.insert(0, 'Oligomer', oligomer)\n",
    "        all_fitted_data_df = pd.concat([all_fitted_data_df, fitted_data_df], ignore_index=True)\n",
    "        \n",
    "        print(f\"Processed and saved plots and data for oligomer {oligomer}\")\n",
    "\n",
    "    # Save all fitted data to an Excel file\n",
    "    all_fitted_data_path = os.path.join(excel_dir, 'all_oligomers_fitted_data.xlsx')\n",
    "    all_fitted_data_df.to_excel(all_fitted_data_path, index=False)\n",
    "\n",
    "    # Normalize and plot the fitted data\n",
    "    for oligomer in all_fitted_data_df['Oligomer'].unique():\n",
    "        oligomer_data = all_fitted_data_df[all_fitted_data_df['Oligomer'] == oligomer]\n",
    "        present_intensity_columns = [col for col in intensity_columns if col in oligomer_data.columns]\n",
    "        max_value = oligomer_data[present_intensity_columns].max().max()\n",
    "        \n",
    "        normalized_df = oligomer_data.copy()\n",
    "        if max_value != 0:\n",
    "            normalized_df[present_intensity_columns] = oligomer_data[present_intensity_columns] / max_value\n",
    "        \n",
    "        all_normalized_data_df = pd.concat([all_normalized_data_df, normalized_df], ignore_index=True)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        max_y_value_normalized = 0\n",
    "        \n",
    "        # Retrieve the assignment for the current oligomer\n",
    "        assignment = assignments.loc[oligomer]\n",
    "\n",
    "        for col in present_intensity_columns:\n",
    "            if col in assignment and normalized_df[col].notna().any():\n",
    "                time = normalized_df['Time']\n",
    "                intensity = normalized_df[col]\n",
    "                monomer = assignment[col]\n",
    "                \n",
    "                time_smooth = np.linspace(time.min(), time.max(), 300)\n",
    "                spl = make_interp_spline(time, intensity, k=3)\n",
    "                intensity_smooth = spl(time_smooth)\n",
    "                \n",
    "                plt.plot(time_smooth, intensity_smooth, label=f'{col}={monomer}', linewidth=line_thickness, color=monomer_colors[monomer])\n",
    "                plt.scatter(time, intensity, marker='o', s=marker_size, color=monomer_colors[monomer])\n",
    "                \n",
    "                max_y_value_normalized = max(max_y_value_normalized, intensity.max())\n",
    "        \n",
    "        plt.xlabel('Time (min)', fontsize=axis_label_font_size)\n",
    "        plt.ylabel('Relative Concentration', fontsize=axis_label_font_size)\n",
    "        plt.legend(title=f'{oligomer}', fontsize=legend_font_size, title_fontsize=legend_font_size)  # Set legend title as the oligomer name\n",
    "        plt.xlim(0, 160)\n",
    "        plt.ylim(-0.05, max_y_value_normalized * 1.1)\n",
    "        plt.xticks(fontsize=axis_value_font_size)\n",
    "        plt.yticks(fontsize=axis_value_font_size)\n",
    "\n",
    "        # Adjust layout and remove white space around the plot\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(normalized_plots_dir, f'{oligomer}_fitted_and_normalized_intensity_over_time.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Processed and saved normalized plots and data for oligomer {oligomer}\")\n",
    "\n",
    "    # Save all normalized data to an Excel file\n",
    "    all_normalized_data_path = os.path.join(normalized_data_dir, 'all_oligomers_fitted_and_normalized_fitted_data.xlsx')\n",
    "    all_normalized_data_df.to_excel(all_normalized_data_path, index=False)\n",
    "\n",
    "    print(f\"All {data_type} plots and data files have been saved successfully.\")\n",
    "\n",
    "# Process both cathodic and anodic data\n",
    "process_oligomer_data('cathodic', cathodic_file_path, cathodic_assignments)\n",
    "process_oligomer_data('anodic', anodic_file_path, anodic_assignments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PCA Analysis on known standards and unknown oligomer data to determine the sequence type of unknown oligomers. Perform sequence matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Define file paths\n",
    "known_file = './PCA Analysis/PCA_Standard_Data.xlsx'\n",
    "unknown_file = './PCA Analysis/Cathodic/Fitted&Normalized Peak Currents/all_oligomers_fitted_and_normalized_fitted_data.xlsx'\n",
    "intensity_columns = ['S1', 'S2', 'S3', 'S4']\n",
    "\n",
    "# Define the base output directory for PCA Analysis\n",
    "pca_analysis_dir = './PCA Analysis'\n",
    "\n",
    "# Ensure that the base output directory exists\n",
    "os.makedirs(pca_analysis_dir, exist_ok=True)\n",
    "\n",
    "# Average conversion factors\n",
    "conversion_factors = {\n",
    "    'S1': 0.929,\n",
    "    'S2': 0.910,\n",
    "    'S3': 0.690\n",
    "}\n",
    "\n",
    "offsets = {\n",
    "    'S1': 0,\n",
    "    'S2': 0,\n",
    "    'S3': 0\n",
    "}\n",
    "\n",
    "def load_and_prepare_data(known_file, unknown_file, intensity_columns):\n",
    "    # Load the known and unknown data\n",
    "    known_df = pd.read_excel(known_file)\n",
    "    unknown_df = pd.read_excel(unknown_file)\n",
    "\n",
    "    # Ensure all required columns exist, fill missing columns with NaN\n",
    "    for col in intensity_columns:\n",
    "        if col not in known_df.columns:\n",
    "            known_df[col] = np.nan\n",
    "        if col not in unknown_df.columns:\n",
    "            unknown_df[col] = np.nan\n",
    "\n",
    "    # Apply conversion factors to the unknown data\n",
    "    for col in intensity_columns[:-1]:  # Exclude 'S4' which has no conversion factor\n",
    "        if col in conversion_factors:\n",
    "            unknown_df[col] = unknown_df[col] * conversion_factors[col] + offsets[col]\n",
    "\n",
    "    # Add intensity count and presence indicators\n",
    "    for df in [known_df, unknown_df]:\n",
    "        df['intensity_count'] = df[intensity_columns].notna().sum(axis=1)\n",
    "        for col in intensity_columns:\n",
    "            df[f'{col}_present'] = df[col].notna().astype(int)\n",
    "\n",
    "    # Aggregate data by 'Oligomer'\n",
    "    def aggregate_data(df):\n",
    "        return df.groupby('Oligomer').agg({\n",
    "            **{col: 'mean' for col in intensity_columns},\n",
    "            **{f'{col}_present': 'mean' for col in intensity_columns},\n",
    "            'intensity_count': 'max'\n",
    "        }).reset_index()\n",
    "\n",
    "    grouped_known_df = aggregate_data(known_df)\n",
    "    grouped_unknown_df = aggregate_data(unknown_df)\n",
    "\n",
    "    # Fill missing intensity values with zero\n",
    "    grouped_known_df[intensity_columns] = grouped_known_df[intensity_columns].fillna(0)\n",
    "    grouped_unknown_df[intensity_columns] = grouped_unknown_df[intensity_columns].fillna(0)\n",
    "\n",
    "    return grouped_known_df, grouped_unknown_df\n",
    "\n",
    "def perform_pca(data, n_components=5):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principal_components = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    return principal_components, pca, scaler\n",
    "\n",
    "def find_best_matches(known_df, unknown_df, principal_components_known, principal_components_unknown):\n",
    "    matches = []\n",
    "\n",
    "    for _, unknown_row in unknown_df.iterrows():\n",
    "        count = unknown_row['intensity_count']\n",
    "        filtered_known = known_df[known_df['intensity_count'] == count]\n",
    "        \n",
    "        # Check if filtered_known is empty\n",
    "        if filtered_known.empty:\n",
    "            print(f\"No known oligomers with intensity count {count} found for unknown oligomer {unknown_row['Oligomer']}. Skipping this oligomer.\")\n",
    "            continue\n",
    "\n",
    "        distances = filtered_known.apply(\n",
    "            lambda row: euclidean(row[['PC1', 'PC2', 'PC3', 'PC4', 'PC5']], unknown_row[['PC1', 'PC2', 'PC3', 'PC4', 'PC5']]),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Find the best match\n",
    "        best_match_idx = distances.idxmin()\n",
    "        best_match = filtered_known.loc[best_match_idx]\n",
    "        \n",
    "        # Find the second-best match, if available\n",
    "        if len(distances) > 1:\n",
    "            second_best_match_idx = distances.drop(best_match_idx).idxmin()\n",
    "            second_best_match = filtered_known.loc[second_best_match_idx]\n",
    "            matches.append({\n",
    "                'Unknown_Oligomer': unknown_row['Oligomer'],\n",
    "                'Best_Match_Oligomer': best_match['Oligomer'],\n",
    "                'Second_Best_Match_Oligomer': second_best_match['Oligomer'],\n",
    "                'Best_Match_Distance': distances.min(),\n",
    "                'Second_Best_Match_Distance': distances.drop(best_match_idx).min()\n",
    "            })\n",
    "        else:\n",
    "            # Only one match available, set second-best to None\n",
    "            matches.append({\n",
    "                'Unknown_Oligomer': unknown_row['Oligomer'],\n",
    "                'Best_Match_Oligomer': best_match['Oligomer'],\n",
    "                'Second_Best_Match_Oligomer': None,\n",
    "                'Best_Match_Distance': distances.min(),\n",
    "                'Second_Best_Match_Distance': None\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(matches)\n",
    "\n",
    "def create_3d_plot(known_df, unknown_df, show_unknown=True):\n",
    "    # Combine known and unknown data\n",
    "    known_df['Type'] = known_df['intensity_count'].apply(lambda x: f'Count_{int(x)}')\n",
    "    if show_unknown:\n",
    "        unknown_df['Type'] = 'Unknown'\n",
    "        combined_df = pd.concat([known_df, unknown_df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = known_df\n",
    "\n",
    "    # Define custom color palette for different intensity counts\n",
    "    color_map = {\n",
    "        'Count_1': '#636EFA',  # Color for intensity count 1\n",
    "        'Count_2': '#009688',  # Color for intensity count 2\n",
    "        'Count_3': '#FFA15A',  # Color for intensity count 3\n",
    "        'Count_4': '#AB63FA',  # Color for intensity count 4\n",
    "        'Unknown': '#EF553B'   # Color for unknown oligomers\n",
    "    }\n",
    "\n",
    "    # Initialize 3D scatter plot with individual traces for each type to ensure labels remain visible\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add each type as a separate trace to control label visibility and color\n",
    "    for type_value in combined_df['Type'].unique():\n",
    "        subset = combined_df[combined_df['Type'] == type_value]\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=subset['PC1'], y=subset['PC2'], z=subset['PC3'],\n",
    "                mode='markers+text',  # Show both markers and text\n",
    "                marker=dict(size=8, color=color_map[type_value], line=dict(width=0)),\n",
    "                text=subset['Oligomer'],  # Display oligomer name\n",
    "                textposition=\"top center\",  # Position text above the marker\n",
    "                textfont=dict(size=20),  # Adjust font size for readability\n",
    "                name=type_value  # Legend label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add drop lines from each point to a fixed z-value (e.g., -2.0)\n",
    "    for i, row in combined_df.iterrows():\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=[row['PC1'], row['PC1']],\n",
    "                y=[row['PC2'], row['PC2']],\n",
    "                z=[row['PC3'], -2.0],\n",
    "                mode='lines',\n",
    "                line=dict(color=color_map[row['Type']], width=6),\n",
    "                showlegend=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout for publication quality\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis=dict(title=dict(font=dict(size=25), text='Principal Component 1'), tickfont=dict(size=16)),\n",
    "            yaxis=dict(title=dict(font=dict(size=25), text='Principal Component 2'), tickfont=dict(size=16)),\n",
    "            zaxis=dict(title=dict(font=dict(size=25), text='Principal Component 3'), tickfont=dict(size=16)),\n",
    "            aspectmode='cube',\n",
    "            camera_eye=dict(x=1.25, y=1.25, z=1.25)\n",
    "        ),\n",
    "        legend=dict(title=dict(text='Intensity Count', font=dict(size=23))),\n",
    "        font=dict(family=\"Arial\", size=23),\n",
    "        margin=dict(l=0, r=0, b=0, t=30),\n",
    "        template='plotly_white',\n",
    "        title='3D PCA of Oligomer Data with Known and Unknown Oligomers' if show_unknown else '3D PCA of Oligomer Data with Known Oligomers'\n",
    "    )\n",
    "\n",
    "    # Define output path for the HTML plot\n",
    "    plot_path = os.path.join(pca_analysis_dir, '3D_PCA_Oligomer_Plot_Publication_Quality.html')\n",
    "    # Save and show the plot\n",
    "    pio.write_html(fig, file=plot_path, auto_open=True)\n",
    "    fig.show()\n",
    "\n",
    "def plot_variance(explained_variance_ratio):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center')\n",
    "    plt.ylabel('Explained variance ratio', fontsize=25)\n",
    "    plt.xlabel('Principal components', fontsize=25)\n",
    "    plt.title('Variance Explained by Each Principal Component', fontsize=23)\n",
    "\n",
    "    # Annotate each bar with the variance ratio\n",
    "    for i, v in enumerate(explained_variance_ratio):\n",
    "        plt.text(i + 1, v, f\"{v:.2f}\", ha='center', va='bottom', fontsize=23)\n",
    "\n",
    "    plt.xticks(fontsize=25)\n",
    "    plt.yticks(fontsize=25)\n",
    "\n",
    "    # Define output path for variance plot\n",
    "    variance_plot_path = os.path.join(pca_analysis_dir, 'Variance_Explained_by_Principal_Components.png')\n",
    "    plt.savefig(variance_plot_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    grouped_known_df, grouped_unknown_df = load_and_prepare_data(known_file, unknown_file, intensity_columns)\n",
    "\n",
    "    # Perform PCA on known data\n",
    "    principal_components_known, pca, scaler = perform_pca(grouped_known_df[intensity_columns + [f'{col}_present' for col in intensity_columns]])\n",
    "\n",
    "    # Transform unknown data using the same PCA model\n",
    "    unknown_X_scaled = scaler.transform(grouped_unknown_df[intensity_columns + [f'{col}_present' for col in intensity_columns]])\n",
    "    principal_components_unknown = pca.transform(unknown_X_scaled)\n",
    "\n",
    "    # Create DataFrame with principal components\n",
    "    known_principal_df = pd.DataFrame(data=principal_components_known, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])\n",
    "    known_result_df = pd.concat([grouped_known_df[['Oligomer', 'intensity_count']], known_principal_df], axis=1)\n",
    "\n",
    "    unknown_principal_df = pd.DataFrame(data=principal_components_unknown, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])\n",
    "    unknown_result_df = pd.concat([grouped_unknown_df[['Oligomer', 'intensity_count']], unknown_principal_df], axis=1)\n",
    "\n",
    "    # Find best matches\n",
    "    matches_df = find_best_matches(known_result_df, unknown_result_df, principal_components_known, principal_components_unknown)\n",
    "\n",
    "    # Save matches to Excel\n",
    "    matches_file_path = os.path.join(pca_analysis_dir, 'oligomer_matches.xlsx')\n",
    "    matches_df.to_excel(matches_file_path, index=False)\n",
    "\n",
    "    # Print matches\n",
    "    print(\"Matches DataFrame:\\n\", matches_df)\n",
    "\n",
    "    # User can decide whether to show unknown oligomers in the 3D plot\n",
    "    show_unknown = False  # Set to False if you do not want to display unknown oligomers\n",
    "    create_3d_plot(known_result_df, unknown_result_df, show_unknown=show_unknown)\n",
    "\n",
    "    # Plot variance\n",
    "    plot_variance(pca.explained_variance_ratio_)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the exact sequence of the analyzed oligomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths for the required Excel files\n",
    "monomer_assignments_path = './Processed Electrochem Data/Monomer_Assignments_Cathodic.xlsx'\n",
    "oligomer_matches_path = './PCA Analysis/oligomer_matches.xlsx'\n",
    "sequences_path = './PCA Analysis/Sequences.xlsx'\n",
    "\n",
    "# Load the Excel files\n",
    "monomer_assignments = pd.read_excel(monomer_assignments_path)\n",
    "oligomer_matches = pd.read_excel(oligomer_matches_path)\n",
    "sequences = pd.read_excel(sequences_path)\n",
    "\n",
    "# Function to reconstruct the exact sequence by replacing placeholders with actual monomers\n",
    "def reconstruct_sequence(sequence_row, oligomer_name, monomer_assignments):\n",
    "    # Extract the placeholder sequence for the oligomer (e.g., S1, S2, S3, S4)\n",
    "    placeholder_sequence = [sequence_row['First'], sequence_row['Second'], sequence_row['Third'], sequence_row['Fourth']]\n",
    "    \n",
    "    # Get the monomer assignments for this oligomer\n",
    "    oligomer_monomer_assignments = monomer_assignments[monomer_assignments['Oligomer'] == oligomer_name]\n",
    "    \n",
    "    # Replace placeholders with actual monomers\n",
    "    reconstructed_sequence = [\n",
    "        oligomer_monomer_assignments[oligomer_monomer_assignments['Assignment'] == placeholder]['Monomer'].values[0]\n",
    "        for placeholder in placeholder_sequence\n",
    "    ]\n",
    "    \n",
    "    return reconstructed_sequence\n",
    "\n",
    "# Iterate over oligomer matches and reconstruct sequences for the best matches\n",
    "reconstructed_sequences = []\n",
    "\n",
    "for _, row in oligomer_matches.iterrows():\n",
    "    oligomer_name = row['Unknown_Oligomer']\n",
    "    \n",
    "    # Get the sequence name for the best match\n",
    "    best_match_seq_name = row['Best_Match_Oligomer']\n",
    "    \n",
    "    # Get the actual sequence using the sequence name\n",
    "    best_match_sequence_row = sequences[sequences['Sequence'] == best_match_seq_name].iloc[0]\n",
    "    \n",
    "    # Reconstruct the sequence for the best match\n",
    "    best_match_reconstructed = reconstruct_sequence(best_match_sequence_row, oligomer_name, monomer_assignments)\n",
    "    \n",
    "    # Save the results with separate columns for First, Second, Third, and Fourth\n",
    "    reconstructed_sequences.append({\n",
    "        'Unknown_Oligomer': oligomer_name,\n",
    "        'Best_Match_Oligomer': best_match_seq_name,\n",
    "        'First': best_match_reconstructed[0],\n",
    "        'Second': best_match_reconstructed[1],\n",
    "        'Third': best_match_reconstructed[2],\n",
    "        'Fourth': best_match_reconstructed[3]\n",
    "    })\n",
    "    \n",
    "    # Print the best match reconstructed sequence\n",
    "    print(f\"Best match reconstructed sequence for {oligomer_name}: {best_match_reconstructed}\")\n",
    "\n",
    "# Convert the results into a dataframe\n",
    "reconstructed_sequences_df = pd.DataFrame(reconstructed_sequences)\n",
    "\n",
    "# Display the reconstructed sequences\n",
    "reconstructed_sequences_df.head()\n",
    "\n",
    "# Save the final reconstructed sequences to an Excel file in the current directory\n",
    "output_file_path = 'best_match_reconstructed_sequences.xlsx'\n",
    "reconstructed_sequences_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Best match reconstructed sequences saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode the ASCII Characters stored by each oligomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths for the required Excel files\n",
    "assigned_information_path = './assigned_information.xlsx'\n",
    "reconstructed_sequences_path = './best_match_reconstructed_sequences.xlsx'\n",
    "\n",
    "# Load the Excel files\n",
    "assigned_information = pd.read_excel(assigned_information_path)\n",
    "reconstructed_sequences_df = pd.read_excel(reconstructed_sequences_path)\n",
    "\n",
    "# Function to match the best match sequence to the possible sequences and find the corresponding ASCII character\n",
    "def find_ascii_for_sequence(row, assigned_information):\n",
    "    # Compare the four monomers with the assigned_information monomers\n",
    "    matched_row = assigned_information[\n",
    "        (assigned_information['First'] == row['First']) &\n",
    "        (assigned_information['Second'] == row['Second']) &\n",
    "        (assigned_information['Third'] == row['Third']) &\n",
    "        (assigned_information['Fourth'] == row['Fourth'])\n",
    "    ]\n",
    "    \n",
    "    if not matched_row.empty:\n",
    "        return matched_row['Encoded ASCII Character'].values[0]  # Return the first match found\n",
    "    return None  # Return None if no match is found\n",
    "\n",
    "# Create a new column to store the ASCII character for each best match reconstructed sequence\n",
    "reconstructed_sequences_df['Encoded_ASCII_Character'] = reconstructed_sequences_df.apply(\n",
    "    lambda row: find_ascii_for_sequence(row, assigned_information), axis=1\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(reconstructed_sequences_df[['Unknown_Oligomer', 'First', 'Second', 'Third', 'Fourth', 'Encoded_ASCII_Character']])\n",
    "\n",
    "# Save the updated dataframe to a new Excel file\n",
    "output_file_path = './reconstructed_sequences_with_ascii.xlsx'\n",
    "reconstructed_sequences_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Reconstructed sequences with ASCII characters saved to {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
